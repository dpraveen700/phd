{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on : https://arxiv.org/abs/1411.2738 for forward and backpropogation derivations of Word2Vec CBOW & Skipgram.\n"
     ]
    }
   ],
   "source": [
    "print (\"Based on : https://arxiv.org/abs/1411.2738 for forward and backpropogation derivations of Word2Vec CBOW & Skipgram.\")\n",
    "#numpy version '1.13.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "Make sure you have internet connection (2-5 Mbps),Tensorflow, math, numpy-1.13.1, NLTK (with data),random, os,six.moves libraries in Python 3.6.3 exists.\n",
      "######################\n",
      "Enter comma seperated words for which you want to predict the nearest words: small,employee\n"
     ]
    }
   ],
   "source": [
    "print (\"######################\")\n",
    "print (\"Make sure you have internet connection (2-5 Mbps),Tensorflow, math, numpy-1.13.1, NLTK (with data),random, os,six.moves libraries in Python 3.6.3 exists.\")\n",
    "print (\"######################\")\n",
    "target_list = input(\"Enter comma seperated words for which you want to predict the nearest words: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "To download pre-trained embeddings(For faster predictions), enter 1, else 0 for training 100000 iterations over data ::1\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "print (\"######################\")\n",
    "download_embed = input(\"To download pre-trained embeddings(For faster predictions), enter 1, else 0 for training 100000 iterations over data::\")\n",
    "print (\"######################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination) \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download embeddings.\n",
    "#https://drive.google.com/file/d/1LiBLVn1Q3-a2GndvnA8-vRlTR9kNBOdE/view?usp=sharing\n",
    "import os.path\n",
    "if(int(download_embed)==1):\n",
    "    id=\"1LiBLVn1Q3-a2GndvnA8-vRlTR9kNBOdE\"\n",
    "    destination=\"skipgram_embed.npy\"\n",
    "    download_file_from_google_drive(id,destination)\n",
    "elif ((os.path.isfile(\"skip_embed.py\")==True) & (int(download_embed)!=1) ):\n",
    "    os.remove(\"skip_embed.py\")\n",
    "else:\n",
    "    print (\"Unknown error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus2word(text):\n",
    "    from keras.preprocessing.text import text_to_word_sequence\n",
    "    # define the document\n",
    "    # tokenize the document\n",
    "    words = text_to_word_sequence(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "The corpus will not be processed as it has been already processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "print (\"######################\")\n",
    "if ((os.path.isfile(\"dictionary.npy\")==True) & (os.path.isfile(\"reverse_dictionary.npy\")==True) & (os.path.isfile(\"words.npy\")==True)):\n",
    "    print(\"The corpus will not be processed as it has been already processed\")\n",
    "else:\n",
    "    print(\"Reading and cleaning corpus to train\")\n",
    "    file = open('corpus.txt', 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    #NLTK for text processing\n",
    "    import nltk\n",
    "    print(\"Removing non english words.\")\n",
    "    eng_words = set(nltk.corpus.words.words())\n",
    "\n",
    "    text_english=\" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
    "             if w.lower() in eng_words or not w.isalpha())\n",
    "    #Text Tokenization\n",
    "    print(\"Split into words\")\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text_english)\n",
    "    print(\"Convert to lower case\")\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    print(\"Remove punctuation from each word\")\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    print(\"Remove remaining tokens that are not alphabetic\")\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    print(\"Filter out stop words\")\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    print(\"Removed Single Letter words\")\n",
    "    single_letters=('b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z')\n",
    "    words=[w for w in words if not w in single_letters]\n",
    "    print(\"Total words in corpus\")\n",
    "    print (len(words))\n",
    "    print(\"Total unique words\")\n",
    "    print (len(set(words)))\n",
    "    np.save(\"words.npy\",words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    vocabulary_size = len(set(words))-1\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "          index = dictionary[word]\n",
    "        else:\n",
    "          index = 0  # dictionary['UNK']\n",
    "          unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    vocabulary = len(set(dictionary))-1\n",
    "    data_in_list=list()\n",
    "    data_in_list.append(data)\n",
    "    return data_in_list, count, dictionary, reverse_dictionary,vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "The corpus will not be processed as it has been already processed\n"
     ]
    }
   ],
   "source": [
    "print (\"######################\")\n",
    "if ((os.path.isfile(\"dictionary.npy\")==True) & (os.path.isfile(\"reverse_dictionary.npy\")==True) & (os.path.isfile(\"data.npy\")==True)):\n",
    "    print(\"The corpus will not be processed as it has been already processed\")\n",
    "else:\n",
    "    data, count, dictionary, reverse_dictionary,V = build_dataset(words)\n",
    "    print('Most common words (+UNK)', count[:5])\n",
    "    print('Sample data', data[:10])\n",
    "    import numpy as np\n",
    "    # Save\n",
    "    np.save('dictionary.npy', dictionary)\n",
    "    np.save('reverse_dictionary.npy', reverse_dictionary)\n",
    "    np.save('data.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "    # Returns\n",
    "        A binary matrix representation of the input.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "def corpus2io(corpus_tokenized, V, window_size):\n",
    "    \"\"\"Converts corpus text into context and center words\n",
    "    # Arguments\n",
    "        corpus_tokenized: corpus text\n",
    "        window_size: size of context window\n",
    "    # Returns\n",
    "        context and center words (arrays)\n",
    "    \"\"\"\n",
    "    for words in corpus_tokenized:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts.append([words[i]-1 for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word-1)\n",
    "            x = np_utils.to_categorical(contexts, V)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculate softmax based probability for given input vector\n",
    "    # Arguments\n",
    "        x: numpy array/list\n",
    "    # Returns\n",
    "        softmax of input array\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(V, N):\n",
    "    \"\"\"\n",
    "    Initialize the weights of the neural network.\n",
    "    :param V: size of the vocabulary\n",
    "    :param N: size of the hidden layer\n",
    "    :return: weights W1, W2\n",
    "    \"\"\"\n",
    "    np.random.seed(100)\n",
    "    W1 = np.random.rand(V, N)\n",
    "    W2 = np.random.rand(N, V)\n",
    "\n",
    "    return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram(context, x, W1, W2, loss):\n",
    "    \"\"\"\n",
    "    Implementation of Skip-Gram Word2Vec model\n",
    "    :param context: all the context words (these represent the labels)\n",
    "    :param label: the center word (this represents the input)\n",
    "    :param W1: weights from the input to the hidden layer\n",
    "    :param W2: weights from the hidden to the output layer\n",
    "    :param loss: float that represents the current value of the loss function\n",
    "    :return: updated weights and loss\n",
    "    \"\"\"\n",
    "    h = np.dot(W1.T, x)\n",
    "    u = np.dot(W2.T, h)\n",
    "    y_pred = softmax(u)\n",
    "\n",
    "    e = np.array([-label + y_pred.T for label in context])\n",
    "    dW2 = np.outer(h, np.sum(e.reshape(e.shape[1],e.shape[2]), axis=0))\n",
    "    dW1 = np.outer(x, np.dot(W2, np.sum(e.reshape(e.shape[1],e.shape[2]), axis=0).T))\n",
    "    new_W1 = W1 - eta * dW1\n",
    "    new_W2 = W2 - eta * dW2\n",
    "    loss += -np.sum([u[label == 1] for label in context.reshape(context.shape[1],context.shape[2])])\n",
    "    return new_W1, new_W2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=np.load(\"words.npy\")\n",
    "corpus_tokenized,_,_,_,V = build_dataset(words)\n",
    "N=150\n",
    "W1, W2 = initialize(V, N)\n",
    "np.random.seed(100)\n",
    "n_epochs=1\n",
    "window_size=2\n",
    "eta = 0.1\n",
    "loss_vs_epoch = []\n",
    "if ((os.path.isfile(\"skipgram_embed.npy\")!=True) & (int(download_embed)!=1)):\n",
    "    for e in range(n_epochs):\n",
    "        loss = 0\n",
    "        for i, (label, center) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "            W1, W2, loss = skipgram(label, center, W1, W2, loss)\n",
    "            loss_vs_epoch.append(loss)\n",
    "    np.save(\"skipgram_embed.npy\", W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict function\n",
    "#Creating predict function\n",
    "#Input: list of words, final_embeddings,reverse_dictionary,top_nearest_words_needed\n",
    "#Output: The nearest 10 words in sorted order.\n",
    "def predicted_words(target_words,final_embeddings,dictionary,reverse_dictionary,top_nearest_words_needed):\n",
    "\n",
    "    #Handling the single input and multiple inputs, both.\n",
    "    target_list=[]\n",
    "    if (type(target_words)==tuple):\n",
    "        for i in range(0,len(target_words)):\n",
    "            target_list.append(target_words[i])\n",
    "    else:\n",
    "        target_list.append(target_words)\n",
    "    \n",
    "    #Take word by word and predict list of nearest words.\n",
    "    for i in range(0,len(target_list)):\n",
    "        #Cleaning target word.\n",
    "        target=target_list[i]\n",
    "        #search for index in dictionary. If not found assign it to unknown word.\n",
    "        #The results will be obviously not accurate, but will not give annoying not found error.\n",
    "        #Approach is debatable! \n",
    "        #But I am following approach in google keypad, when I enter non existing word, it will still give some junk predictions.\n",
    "        if(dictionary.get(target)==None):\n",
    "            target_embedding=final_embeddings[0,:]\n",
    "        else:\n",
    "            target_embedding=final_embeddings[int(dictionary.get(target))-1,:]\n",
    "        \n",
    "        if(dictionary.get(target)!=None):\n",
    "            \n",
    "            #cosine_similarity\n",
    "            cosine_similarity=np.matmul(target_embedding,np.transpose(final_embeddings[0:final_embeddings.shape[0],:]))\n",
    "        \n",
    "            #I am building a array with index in one column and cosine similarity in another column.\n",
    "            word_index=np.zeros((cosine_similarity.shape[0],1), dtype=int)\n",
    "        \n",
    "            for j in range(0,cosine_similarity.shape[0]):\n",
    "                word_index[j]=j+1\n",
    "        \n",
    "            #Reshaping cosine_similarity to match with word index array.\n",
    "            cosine_similarity_reshape=cosine_similarity.reshape(cosine_similarity.shape[0],1)\n",
    "        \n",
    "            #Appending & Sorting.\n",
    "            target_sim_array=np.append(word_index,cosine_similarity_reshape,axis=1)\n",
    "            target_sim_array_sorted=target_sim_array[target_sim_array[:, 1].argsort()]\n",
    "        \n",
    "            #Taking top nearest word index except word itself (0 th position)\n",
    "            top_word_index=target_sim_array_sorted[:,0][-(top_nearest_words_needed+1):-1:]\n",
    "        \n",
    "            #Top word predictions using reverse array and reverse dictionary\n",
    "            predicted_words=list()\n",
    "            for k in reversed(top_word_index):\n",
    "                predicted_words.append(reverse_dictionary[k])\n",
    "            \n",
    "            print (\"Nearest words for word: %s\" %target, \": ordered by nearest word predicted first is %s\" %predicted_words)\n",
    "        else:\n",
    "                      \n",
    "            print (\"unknown word: %s\" %target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load embeddings, dictionary and reverse dictionary saved earlier in files.\n",
    "# Advantage is you can reduce run time, if these files doesn't exist.\n",
    "final_embeddings = np.load(\"skipgram_embed.npy\")\n",
    "dictionary = np.load('dictionary.npy').item()\n",
    "reverse_dictionary = np.load('reverse_dictionary.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_words=(\"small\",\"employee\")\n",
    "top_nearest_words_needed=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "Nearest words for word: small : ordered by nearest word predicted first is ['krypton', 'range', 'platform', 'demodulator', 'trident', 'piece', 'need', 'unit', 'faro', 'magnetically']\n",
      "Nearest words for word: employee : ordered by nearest word predicted first is ['speedy', 'insanity', 'hurry', 'ame', 'synthetically', 'balk', 'ruth', 'hurdle', 'gade', 'whitish']\n"
     ]
    }
   ],
   "source": [
    "print (\"###########################\")\n",
    "predicted_words(target_words,final_embeddings,dictionary,reverse_dictionary,top_nearest_words_needed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
