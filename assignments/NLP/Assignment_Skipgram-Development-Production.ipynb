{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the target words in comma seperated list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "Make sure you have internet connection (2-5 Mbps),Tensorflow, math, numpy, NLTK (with data),random, os,six.moves libraries in Python 3.6.3 exists.\n",
      "######################\n",
      "Enter comma seperated words for which you want to predict the nearest words: small\n"
     ]
    }
   ],
   "source": [
    "print (\"######################\")\n",
    "print (\"Make sure you have internet connection (2-5 Mbps),Tensorflow, math, numpy, NLTK (with data),random, os,six.moves libraries in Python 3.6.3 exists.\")\n",
    "print (\"######################\")\n",
    "target_list = input(\"Enter comma seperated words for which you want to predict the nearest words: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "To download pre-trained embeddings(For faster predictions), enter 1, else 0 for training 100000 iterations over data ::1\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "print (\"######################\")\n",
    "download_embed = input(\"To download pre-trained embeddings(For faster predictions), enter 1, else 0 for training 100000 iterations over data ::\")\n",
    "print (\"######################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing inputs in to words\n",
      "['small']\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing inputs in to words\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "target_tokens = word_tokenize(target_list)\n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to lower case\n",
      "['small']\n"
     ]
    }
   ],
   "source": [
    "print (\"Convert to lower case\")\n",
    "target_tokens = [w.lower() for w in target_tokens]\n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove punctuation from each word, if any ['small']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "target_stripped = [w.translate(table) for w in target_tokens]\n",
    "print (\"remove punctuation from each word, if any\",target_stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove remaining tokens that are not alphabetic, if any ['small']\n"
     ]
    }
   ],
   "source": [
    "# remove remaining tokens that are not alphabetic\n",
    "target_words = [word for word in target_stripped if word.isalpha()]\n",
    "print (\"remove remaining tokens that are not alphabetic, if any\",target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting the target_words to tuple as that is what the further functions expect.\n",
    "target_words=tuple(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run these. \n",
    "#Still there are many more libraries which I have placed in code in appropriate sections for ease of readability.\n",
    "import os\n",
    "import random\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Corpus to train.\n",
    "\n",
    "    The Data placed in your Google Drive and shared. You can get the id from URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination) \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download the file if not yet downloaded.\n",
    "   #https://drive.google.com/file/d/1rI79RgJx2ZajOGtYsw-kxCjrr2VUYH1e/view?usp=sharing\n",
    "   #id=1rI79RgJx2ZajOGtYsw-kxCjrr2VUYH1e\n",
    "import os.path\n",
    "if((os.path.isfile(\"corpus.txt\")!=True)):\n",
    "   id=\"1rI79RgJx2ZajOGtYsw-kxCjrr2VUYH1e\"\n",
    "   destination=\"corpus.txt\"\n",
    "   download_file_from_google_drive(id,destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download embeddings.\n",
    "#https://drive.google.com/file/d/1LiBLVn1Q3-a2GndvnA8-vRlTR9kNBOdE/view?usp=sharing\n",
    "import os.path\n",
    "if(int(download_embed)==1):\n",
    "    id=\"1LiBLVn1Q3-a2GndvnA8-vRlTR9kNBOdE\"\n",
    "    destination=\"skip_embed.py\"\n",
    "    download_file_from_google_drive(id,destination)\n",
    "elif ((os.path.isfile(\"skip_embed.py\")==True) & (int(download_embed)!=1) ):\n",
    "    os.remove(\"skip_embed.py\")\n",
    "else:\n",
    "    print (\"Unknown error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Reading and Cleaning Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "The corpus will not be processed as it has been already processed\n"
     ]
    }
   ],
   "source": [
    "print (\"######################\")\n",
    "if ((os.path.isfile(\"dictionary.npy\")==True) & (os.path.isfile(\"reverse_dictionary.npy\")==True)):\n",
    "    print(\"The corpus will not be processed as it has been already processed\")\n",
    "else:\n",
    "    print(\"Reading and cleaning corpus to train\")\n",
    "    file = open('corpus.txt', 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    #NLTK for text processing\n",
    "    import nltk\n",
    "    print(\"Removing non english words.\")\n",
    "    eng_words = set(nltk.corpus.words.words())\n",
    "\n",
    "    text_english=\" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
    "             if w.lower() in eng_words or not w.isalpha())\n",
    "    #Text Tokenization\n",
    "    print(\"Split into words\")\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text_english)\n",
    "    print(\"Convert to lower case\")\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    print(\"Remove punctuation from each word\")\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    print(\"Remove remaining tokens that are not alphabetic\")\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    print(\"Filter out stop words\")\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    print(\"Removed Single Letter words\")\n",
    "    single_letters=('b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z')\n",
    "    words=[w for w in words if not w in single_letters]\n",
    "    print(\"Total words in corpus\")\n",
    "    print (len(words))\n",
    "    print(\"Total unique words\")\n",
    "    print (len(set(words)))\n",
    "    #Word Dictionary and Anlysis of the words.\n",
    "    vocabulary_size = len(set(words))\n",
    "    import collections\n",
    "\n",
    "    def build_dataset(words):\n",
    "      count = [['UNK', -1]]\n",
    "      count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "      dictionary = dict()\n",
    "      for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "      data = list()\n",
    "      unk_count = 0\n",
    "      for word in words:\n",
    "        if word in dictionary:\n",
    "          index = dictionary[word]\n",
    "        else:\n",
    "          index = 0  # dictionary['UNK']\n",
    "          unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "      count[0][1] = unk_count\n",
    "      reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "      return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "    print('Most common words (+UNK)', count[:5])\n",
    "    print('Sample data', data[:10])\n",
    "    import numpy as np\n",
    "    # Save\n",
    "    np.save('dictionary.npy', dictionary)\n",
    "    np.save('reverse_dictionary.npy', reverse_dictionary)\n",
    "    np.save('data.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "Loading Data, Dictionary & Reverse Dictionary:\n"
     ]
    }
   ],
   "source": [
    "print (\"######################\")\n",
    "print (\"Loading Data, Dictionary & Reverse Dictionary:\")\n",
    "data=(list((np.load(\"data.npy\"))))\n",
    "dictionary = np.load('dictionary.npy').item()\n",
    "reverse_dictionary = np.load('reverse_dictionary.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indexed Dictionary, Reverse Dictionary, Analysis of Frequently Used Word Counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Generation\n",
    "    \n",
    "    Batches of data will be fed in each iteration of tensorflow. Every iteration will get new batch of data, which is controled by global data_index variable, which gets updated in each run of generate_batch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['selected', 'eyewear', 'noise', 'insertion']\n",
      "\n",
      " Showing how the skip windows and skips transform training data: \n",
      "\n",
      "\n",
      "with num_skips = 1 and skip_window = 1:\n",
      "    features: ['eyewear', 'noise', 'insertion', 'loss']\n",
      "    labels: ['selected', 'eyewear', 'loss', 'insertion']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#This is a important variable. This gets updated everytime batch function is run at global context.\n",
    "#Basically data_index after one run, from where of data I need to select the next batch.\n",
    "#So in tf for each iteration when you call batch you will get unique batch till the end of the data.\n",
    "data_index = 0\n",
    "#Creating multiple batches of rows=batch_size for batch learning.\n",
    "#We will be calling this and passing batch size while calling TF session.\n",
    "\n",
    "#Please note: This function is getting \"data\" from earlier defined and populated data variable.\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the Center.\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "#The sample of data.\n",
    "print('data:', [reverse_dictionary[di] for di in data[:4]])\n",
    "\n",
    "print (\"\\n Showing how the skip windows and skips transform training data: \\n\")\n",
    "for num_skips, skip_window in [(1, 1)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=4, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    features:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining batch_size, embedding size(word vector size), skip window, negative sample size.\n",
    "batch_size = 150\n",
    "embedding_size = 150 # Dimension of the embedding vector.\n",
    "skip_window = 1 # words to consider left and right.\n",
    "num_skips = 1 # How many times to reuse an input to generate a label\n",
    "num_sampled = 80 # Number of negative examples to sample.\n",
    "vocabulary_size=len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tensorflow Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "        # Input data.\n",
    "        train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "        # Variables.\n",
    "        # embedding, vector for each word in the vocabulary\n",
    "        #Remember that you have defined vocabulary_size = 7 when you created function build dataset\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        \n",
    "        softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        \n",
    "        softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        \n",
    "        # Looking up for embeddings for train dataset\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "\n",
    "        # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "        # inputs are embeddings of the train words\n",
    "        # with this loss we optimize weights, biases, embeddings\n",
    "        \n",
    "        \n",
    "        #loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=avg_embed,\n",
    "         #                      labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "            #After doing the literature study, found out nce_loss is better than sampled_softmax_loss.\n",
    "        # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=softmax_weights,\n",
    "                 biases=softmax_biases,\n",
    "                 labels=train_labels,\n",
    "                 inputs=embed,\n",
    "                 num_sampled=num_sampled,\n",
    "                 num_classes=vocabulary_size))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using Tensorflow Session on Batches of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already embedding file trained earlier/downloaded exists. Skipping training process.\n"
     ]
    }
   ],
   "source": [
    "#Run\n",
    "#Iterations\n",
    "import os.path\n",
    "num_steps = 100000\n",
    "if ((os.path.isfile(\"skip_embed.npy\")!=True)):\n",
    "    print (\"########################\")\n",
    "    print(\"We will run Tensor Flow session and create & save embeddings in embed.py, as it doesn't exist.\\n Will take more than 10-15 minutes for 100000 iterations, depending upon your system config. Please wait for predictions. \\n\")\n",
    "    print (\"########################\")\n",
    "    with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Session Initialized')\n",
    "            average_loss = 0\n",
    "            for step in range(num_steps):\n",
    "                batch_data, batch_labels = generate_batch(batch_size, num_skips=num_skips, skip_window=skip_window)\n",
    "                feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "                #I am only interested to get loss in return in variable 1 and add it to average loss.\n",
    "                _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                average_loss += l\n",
    "                if step % 2000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss = average_loss / 2000\n",
    "                        # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                    print('Average loss at step %d: %f' % (step, average_loss))\n",
    "                    average_loss = 0\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 stepss\n",
    "                final_embeddings = normalized_embeddings.eval()\n",
    "                np.save(\"skip_embed.npy\", final_embeddings)\n",
    "else:\n",
    "    print (\"Already embedding file trained earlier/downloaded exists. Skipping training process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating predict function\n",
    "#Input: list of words, final_embeddings,reverse_dictionary,top_nearest_words_needed\n",
    "#Output: The nearest 10 words in sorted order.\n",
    "def predicted_words(target_words,final_embeddings,dictionary,reverse_dictionary,top_nearest_words_needed):\n",
    "\n",
    "    #Handling the single input and multiple inputs, both.\n",
    "    target_list=[]\n",
    "    if (type(target_words)==tuple):\n",
    "        for i in range(0,len(target_words)):\n",
    "            target_list.append(target_words[i])\n",
    "    else:\n",
    "        target_list.append(target_words)\n",
    "    \n",
    "    #Take word by word and predict list of nearest words.\n",
    "    for i in range(0,len(target_list)):\n",
    "        #Cleaning target word.\n",
    "        target=target_list[i]\n",
    "        #search for index in dictionary. If not found assign it to unknown word.\n",
    "        #The results will be obviously not accurate, but will not give annoying not found error.\n",
    "        #Approach is debatable! \n",
    "        #But I am following approach in google keypad, when I enter non existing word, it will still give some junk predictions.\n",
    "        if(dictionary.get(target)==None):\n",
    "            target_embedding=final_embeddings[0,:]\n",
    "        else:\n",
    "            target_embedding=final_embeddings[int(dictionary.get(target)),:]\n",
    "        \n",
    "        if(dictionary.get(target)!=None):\n",
    "            #Avoiding Comparing Similarity with Unknown Vector\n",
    "            #cosine_similarity\n",
    "            cosine_similarity=np.matmul(target_embedding,np.transpose(final_embeddings[1:final_embeddings.shape[0],:]))\n",
    "        \n",
    "            #I am building a array with index in one column and cosine similarity in another column.\n",
    "            word_index=np.zeros((cosine_similarity.shape[0],1), dtype=int)\n",
    "        \n",
    "            for j in range(0,cosine_similarity.shape[0]):\n",
    "                word_index[j]=j+1\n",
    "        \n",
    "            #Reshaping cosine_similarity to match with word index array.\n",
    "            cosine_similarity_reshape=cosine_similarity.reshape(cosine_similarity.shape[0],1)\n",
    "        \n",
    "            #Appending & Sorting.\n",
    "            target_sim_array=np.append(word_index,cosine_similarity_reshape,axis=1)\n",
    "            target_sim_array_sorted=target_sim_array[target_sim_array[:, 1].argsort()]\n",
    "        \n",
    "            #Taking top nearest word index except word itself (0 th position)\n",
    "            top_word_index=target_sim_array_sorted[:,0][-(top_nearest_words_needed+1):-1:]\n",
    "        \n",
    "            #Top word predictions using reverse array and reverse dictionary\n",
    "            predicted_words=list()\n",
    "            for k in reversed(top_word_index):\n",
    "                predicted_words.append(reverse_dictionary[k])\n",
    "            \n",
    "            print (\"\\n Nearest words for word: %s\" %target, \": ordered by nearest word predicted first is %s\" %predicted_words)\n",
    "        else:\n",
    "            \n",
    "            #cosine_similarity\n",
    "            cosine_similarity=np.matmul(final_embeddings[0,:],np.transpose(final_embeddings))\n",
    "        \n",
    "            #I am building a array with index in one column and cosine similarity in another column.\n",
    "            word_index=np.zeros((cosine_similarity.shape[0],1), dtype=int)\n",
    "        \n",
    "            for j in range(0,cosine_similarity.shape[0]):\n",
    "                word_index[j]=j\n",
    "        \n",
    "            #Reshaping cosine_similarity to match with word index array.\n",
    "            cosine_similarity_reshape=cosine_similarity.reshape(cosine_similarity.shape[0],1)\n",
    "        \n",
    "            #Appending & Sorting.\n",
    "            target_sim_array=np.append(word_index,cosine_similarity_reshape,axis=1)\n",
    "            target_sim_array_sorted=target_sim_array[target_sim_array[:, 1].argsort()]\n",
    "        \n",
    "            #Taking top nearest word index except word itself (0 th position)\n",
    "            top_word_index=target_sim_array_sorted[:,0][-(top_nearest_words_needed+1):-1:]\n",
    "        \n",
    "            #Top word predictions using reverse array and reverse dictionary\n",
    "            predicted_words=list()\n",
    "            for k in reversed(top_word_index):\n",
    "                predicted_words.append(reverse_dictionary[k])\n",
    "            \n",
    "            print (\"\\n The word is unknown as of now. Still, based on cosine distance for unknown word vector, Nearest for words for unknown word: %s\" %target, \": ordered by nearest word predicted first is %s\" %predicted_words)  \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('small',)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "top_nearest_words_needed=10\n",
    "# Load embeddings, dictionary and reverse dictionary saved earlier in files.\n",
    "# Advantage is you can reduce run time, if these files doesn't exist.\n",
    "final_embeddings=np.load(\"skip_embed.npy\")\n",
    "dictionary = np.load('dictionary.npy').item()\n",
    "reverse_dictionary = np.load('reverse_dictionary.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "\n",
      " Nearest words for word: small : ordered by nearest word predicted first is ['large', 'asperity', 'although', 'abalone', 'isochrone', 'electroshock', 'smaller', 'unbound', 'leat', 'jinja']\n"
     ]
    }
   ],
   "source": [
    "print (\"###########################\")\n",
    "predicted_words(target_words,final_embeddings,dictionary,reverse_dictionary,top_nearest_words_needed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
