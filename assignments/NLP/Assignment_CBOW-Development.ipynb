{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note for Production Implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If embeddings,dictionary and reverser_dictionary files already exist, then don't train again. \n",
    "#Typically useful for running the code second time.\n",
    "#Else run the trainings and return the predictions.\n",
    "\n",
    "import os.path\n",
    "print ((os.path.isfile(\"embed.npy\")==True) & (os.path.isfile(\"dictionary.npy\")==True) & (os.path.isfile(\"reverse_dictionary.npy\")==True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run these. Still there are many more libraries which I have placed in code in appropriate sections for ease of readability.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Corpus to train.\n",
    "\n",
    "    The Data placed in your Google Drive and shared. You can get the id from URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination) \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the file.\n",
    "#https://drive.google.com/file/d/1rI79RgJx2ZajOGtYsw-kxCjrr2VUYH1e/view?usp=sharing\n",
    "#id=1rI79RgJx2ZajOGtYsw-kxCjrr2VUYH1e\n",
    "id=\"1rI79RgJx2ZajOGtYsw-kxCjrr2VUYH1e\"\n",
    "destination=\"corpus.txt\"\n",
    "download_file_from_google_drive(id,destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Reading and Cleaning Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('corpus.txt', 'r')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK for text processing\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing non english words.\n",
    "eng_words = set(nltk.corpus.words.words())\n",
    "\n",
    "text_english=\" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
    "         if w.lower() in eng_words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_english[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Cleansing\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removed Single Letter words\n",
    "single_letters=('b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z')\n",
    "words=[w for w in words if not w in single_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total word size\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total unique words, your vocabulary\n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indexed Dictionary, Reverse Dictionary, Analysis of Frequently Used Word Counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Dictionary and Anlysis of the words.\n",
    "vocabulary_size = len(set(words))\n",
    "import collections\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    " \n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "#del words  # Will help to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Dictionary & Reverse Dictionary in External File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Save\n",
    "np.save('dictionary.npy', dictionary)\n",
    "np.save('reverse_dictionary.npy', reverse_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Generation\n",
    "    \n",
    "    Batches of data will be fed in each iteration of tensorflow. Every iteration will get new batch of data, which is controled by global data_index variable, which gets updated in each run of generate_batch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#This is a important variable. This gets updated everytime batch function is run at global context.\n",
    "#Basically data_index after one run, from where of data I need to select the next batch.\n",
    "#So in tf for each iteration when you call batch you will get unique batch till the end of the data.\n",
    "data_index = 0\n",
    "#Creating multiple batches of rows=batch_size for batch learning.\n",
    "#We will be calling this and passing batch size while calling TF session.\n",
    "\n",
    "#This function is getting data from earlier defined and populated data variable.\n",
    "\n",
    "def generate_batch(batch_size, skip_window):\n",
    "    # skip window is the amount of words we're looking at from each side of a given word\n",
    "    # creates a single batch\n",
    "    \n",
    "    #This is a important variable. This gets updated everytime batch function is run at global context.\n",
    "    #Basically data_index after one run, from where of data I need to select the next batch.\n",
    "    #So in tf for each iteration when you call batch you will get unique batch till the end of the data.\n",
    "    global data_index\n",
    "\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size,span-1), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    # e.g if skip_window = 2 then span = 5\n",
    "    # span is the length of the whole frame we are considering for a single word (left + word + right)\n",
    "    # skip_window is the length of one side\n",
    "\n",
    "    # queue which add and pop at the end\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "\n",
    "    #get words starting from index 0 to span\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        target_to_avoid = [ skip_window ] # we only need to know the words around a given word, not the word itself\n",
    "\n",
    "        # add selected target to avoid_list for next time\n",
    "        col_idx = 0\n",
    "        for j in range(span):\n",
    "            if j==span//2:\n",
    "                continue\n",
    "            # e.g. i=0, j=0 => 0; i=0,j=1 => 1; i=1,j=0 => 2\n",
    "            batch[i,col_idx] = buffer[j] # [skip_window] => middle element\n",
    "            col_idx += 1\n",
    "        labels[i, 0] = buffer[target]\n",
    "\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    assert batch.shape[0]==batch_size and batch.shape[1]== span-1\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "#Checking if everything is working fine.\n",
    "#Number of skips is useless here and nowhere used in CBOW generating dataset. Left over code from Skipgram implementation.\n",
    "\n",
    "for num_skips, skip_window in [(1, 1)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=4, skip_window=skip_window)\n",
    "    print('\\nwith skip_window = %d:' % (skip_window))\n",
    "    \n",
    "    print('    batch:', [[reverse_dictionary[bii] for bii in bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining batch_size, embedding size(word vector size), skip window, negative sample size.\n",
    "batch_size = 150\n",
    "embedding_size = 150 # Dimension of the embedding vector.\n",
    "skip_window = 1 # words to consider left and right.\n",
    "num_sampled = 50 # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tensorflow Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "        # Input data.\n",
    "        train_dataset = tf.placeholder(tf.int32, shape=[batch_size,2*skip_window])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "        # Variables.\n",
    "        # embedding, vector for each word in the vocabulary\n",
    "        #Remember that you have defined vocabulary_size = 7 when you created function build dataset\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        \n",
    "        softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        \n",
    "        softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        # Averaging embeddings accross the full context into a single embedding layer\n",
    "        context_embeddings = []\n",
    "        \n",
    "        for i in range(2*skip_window):\n",
    "            \n",
    "            context_embeddings.append(tf.nn.embedding_lookup(embeddings, train_dataset[:,i]))\n",
    "            \n",
    "        avg_embed =  tf.reduce_mean(tf.stack(axis=0,values=context_embeddings),0,keep_dims=False)\n",
    "\n",
    "        # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "        # inputs are embeddings of the train words\n",
    "        # with this loss we optimize weights, biases, embeddings\n",
    "\n",
    "        #loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=avg_embed,\n",
    "         #                      labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=softmax_weights,\n",
    "                 biases=softmax_biases,\n",
    "                 labels=train_labels,\n",
    "                 inputs=avg_embed,\n",
    "                 num_sampled=num_sampled,\n",
    "                 num_classes=vocabulary_size))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using Tensorflow Session on Batches of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run\n",
    "#Iterations\n",
    "num_steps = 100000\n",
    "with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Session Initialized')\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = generate_batch(batch_size, skip_window)\n",
    "            feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "            #I am only interested to get loss in return in variable 1 and add it to average loss.\n",
    "            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            average_loss += l\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss = average_loss / 2000\n",
    "                    # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step %d: %f' % (step, average_loss))\n",
    "                average_loss = 0\n",
    "            # note that this is expensive (~20% slowdown if computed every 500 stepss\n",
    "            final_embeddings = normalized_embeddings.eval()\n",
    "            np.save(\"embed.npy\", final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating predict function\n",
    "#Input: list of words, final_embeddings,reverse_dictionary,top_nearest_words_needed\n",
    "#Output: The nearest 10 words in sorted order.\n",
    "def predicted_words(target_words,final_embeddings,dictionary,reverse_dictionary,top_nearest_words_needed):\n",
    "\n",
    "    #Handling the single input and multiple inputs, both.\n",
    "    target_list=[]\n",
    "    if (type(target_words)==tuple):\n",
    "        for i in range(0,len(target_words)):\n",
    "            target_list.append(target_words[i])\n",
    "    else:\n",
    "        target_list.append(target_words)\n",
    "    \n",
    "    #Take word by word and predict list of nearest words.\n",
    "    for i in range(0,len(target_list)):\n",
    "        #Cleaning target word.\n",
    "        target=target_list[i]\n",
    "        #search for index in dictionary. If not found assign it to unknown word.\n",
    "        #The results will be obviously not accurate, but will not give annoying not found error.\n",
    "        #Approach is debatable! \n",
    "        #But I am following approach in google keypad, when I enter non existing word, it will still give some junk predictions.\n",
    "        if(dictionary.get(target)==None):\n",
    "            target_embedding=final_embeddings[0,:]\n",
    "        else:\n",
    "            target_embedding=final_embeddings[int(dictionary.get(target)),:]\n",
    "        \n",
    "        if(dictionary.get(target)!=None):\n",
    "            #Avoiding Comparing Similarity with Unknown Vector\n",
    "            #cosine_similarity\n",
    "            cosine_similarity=np.matmul(target_embedding,np.transpose(final_embeddings[1:final_embeddings.shape[0],:]))\n",
    "        \n",
    "            #I am building a array with index in one column and cosine similarity in another column.\n",
    "            word_index=np.zeros((cosine_similarity.shape[0],1), dtype=int)\n",
    "        \n",
    "            for j in range(0,cosine_similarity.shape[0]):\n",
    "                word_index[j]=j+1\n",
    "        \n",
    "            #Reshaping cosine_similarity to match with word index array.\n",
    "            cosine_similarity_reshape=cosine_similarity.reshape(cosine_similarity.shape[0],1)\n",
    "        \n",
    "            #Appending & Sorting.\n",
    "            target_sim_array=np.append(word_index,cosine_similarity_reshape,axis=1)\n",
    "            target_sim_array_sorted=target_sim_array[target_sim_array[:, 1].argsort()]\n",
    "        \n",
    "            #Taking top nearest word index except word itself (0 th position)\n",
    "            top_word_index=target_sim_array_sorted[:,0][-(top_nearest_words_needed+1):-1:]\n",
    "        \n",
    "            #Top word predictions using reverse array and reverse dictionary\n",
    "            predicted_words=list()\n",
    "            for k in reversed(top_word_index):\n",
    "                predicted_words.append(reverse_dictionary[k])\n",
    "            \n",
    "            print (\"Nearest words for word: %s\" %target, \": ordered by nearest word predicted first is %s\" %predicted_words)\n",
    "        else:\n",
    "            \n",
    "            #cosine_similarity\n",
    "            cosine_similarity=np.matmul(final_embeddings[0,:],np.transpose(final_embeddings))\n",
    "        \n",
    "            #I am building a array with index in one column and cosine similarity in another column.\n",
    "            word_index=np.zeros((cosine_similarity.shape[0],1), dtype=int)\n",
    "        \n",
    "            for j in range(0,cosine_similarity.shape[0]):\n",
    "                word_index[j]=j\n",
    "        \n",
    "            #Reshaping cosine_similarity to match with word index array.\n",
    "            cosine_similarity_reshape=cosine_similarity.reshape(cosine_similarity.shape[0],1)\n",
    "        \n",
    "            #Appending & Sorting.\n",
    "            target_sim_array=np.append(word_index,cosine_similarity_reshape,axis=1)\n",
    "            target_sim_array_sorted=target_sim_array[target_sim_array[:, 1].argsort()]\n",
    "        \n",
    "            #Taking top nearest word index except word itself (0 th position)\n",
    "            top_word_index=target_sim_array_sorted[:,0][-(top_nearest_words_needed+1):-1:]\n",
    "        \n",
    "            #Top word predictions using reverse array and reverse dictionary\n",
    "            predicted_words=list()\n",
    "            for k in reversed(top_word_index):\n",
    "                predicted_words.append(reverse_dictionary[k])\n",
    "            \n",
    "            print (\"The word is unknown as of now. Still, based on cosine distance for unknown word vector, Nearest for words for unknown word: %s\" %target, \": ordered by nearest word predicted first is %s\" %predicted_words)            \n",
    "        \n",
    "        #Top word predictions using reverse dictionary\n",
    "    #for i in range(0,top_word_index.shape[0]):\n",
    "        #print (\"Nearest words for %s\" %target, \"based on nearest first is %s\" %reverse_dictionary[top_word_index[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enter Target words in comma seperated string manner as below.\n",
    "target_words=(\"small\",\"employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_nearest_words_needed=10\n",
    "# Load embeddings, dictionary and reverse dictionary saved earlier in files.\n",
    "# Advantage is you can reduce run time, if these files doesn't exist.\n",
    "final_embeddings=np.load(\"embed.npy\")\n",
    "dictionary = np.load('dictionary.npy').item()\n",
    "reverse_dictionary = np.load('reverse_dictionary.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_words(target_words,final_embeddings,dictionary,reverse_dictionary,top_nearest_words_needed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
